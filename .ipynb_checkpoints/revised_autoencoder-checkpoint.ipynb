{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_row', None)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.offline as pyo\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from plotly.tools import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "from   plotly.graph_objs import *\n",
    "from   plotly import tools\n",
    "from   plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "pyo.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import regularizers\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "# \n",
    "from sklearn.preprocessing import  StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "import pymysql\n",
    "\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_100(number):\n",
    "    return number *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start_calculation():  \n",
    "\n",
    "    #sqlEngine       = create_engine(connection)\n",
    "\n",
    "    #dbConnection    = sqlEngine.connect()\n",
    "\n",
    "    df = pd.read_csv(\"YOUR_PATH_TO_OUTLIERS_FILE.csv\", header=0, index_col='author_idauthor',\n",
    "                     usecols = ['author_idauthor', 'tReg','tReg1','tReg2','tReg3','tReg4',\n",
    "                               'tReg5', 'tReg6', 'tReg7', 'tReg8', 'tReg9', 'tReg10', 'tReg11', \n",
    "                                'tReg12','tReg13','hasListing', 'hasListing1', 'hasListing2', \n",
    "                                'hasListing3', 'hasListing4', 'hasListing5', 'hasListing6', 'hasListing7', \n",
    "                                'hasListing8', 'hasListing9', 'hasListing10', 'hasListing11','hasListing12', \n",
    "                                'hasListing13','hasReview', 'hasReview1', 'hasReview2', 'hasReview3', 'hasReview4', \n",
    "                                'hasReview5', 'hasReview6', 'hasReview7', 'hasReview8', 'hasReview9', 'hasReview10', \n",
    "                                'hasReview11', 'hasReview12','hasReview13',\n",
    "                                'isMask', 'isMask1', 'isMask2', 'isMask3', 'isMask4', 'isMask5', 'isMask6', 'isMask7', \n",
    "                                'isMask8', 'isMask9', 'isMask10', 'isMask11', 'isMask12', 'isMask13','followingDelta', 'followingDelta1', 'followingDelta2', \n",
    "                                'followingDelta3', 'followingDelta4', 'followingDelta5', 'followingDelta6', \n",
    "                                'followingDelta7', 'followingDelta8', 'followingDelta9', 'followingDelta10', \n",
    "                                'followingDelta11', 'followingDelta12', 'followingDelta13', 'status', 'status1', \n",
    "                                'status2', 'status3', 'status4', 'status5', 'status6', 'status7', 'status8', \n",
    "                                'status9', 'status10', 'status11', 'status12', 'status13', 'successTrade', \n",
    "                                'successTrade1', 'successTrade2', 'successTrade3', 'successTrade4', \n",
    "                                'successTrade5', 'successTrade6', 'successTrade7', 'successTrade8', \n",
    "                                'successTrade9', 'successTrade10', 'successTrade11', 'successTrade12', 'successTrade13', \n",
    "                                'rating', 'rating1', 'rating2', 'rating3', 'rating4',\n",
    "                                'rating5', 'rating6', 'rating7', 'rating8', 'rating9', 'rating10', 'rating11', \n",
    "                                'rating12', 'rating13', 'unresolvedTxn', 'unresolvedTxn1', 'unresolvedTxn2', \n",
    "                                'unresolvedTxn3', 'unresolvedTxn4', 'unresolvedTxn5', 'unresolvedTxn6', 'unresolvedTxn7', \n",
    "                                'unresolvedTxn8', 'unresolvedTxn9', 'unresolvedTxn10', 'unresolvedTxn11', 'unresolvedTxn12',\n",
    "                                'unresolvedTxn13', 'label'])\n",
    "    print(\"df\", df.shape, df.size, df.ndim, df.columns ,df.isna().sum() )\n",
    "    #print(\"\\n\")\n",
    "    #print(df.head())\n",
    "    df[['followersDelta1*100']] = df[['followingDelta1']].apply(multiply_by_100)\n",
    "    y_true = df['label']\n",
    "    df = df[df['label'] ==0 ]\n",
    "    print(\"df\", df.shape, df.size, df.ndim, df.columns ,df.isna().sum() )\n",
    "    #input()\n",
    "    #print('after generate new feature', df.head())\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    #print(df)\n",
    "    #dbConnection.close()\n",
    "    \n",
    "    RANDOM_SEED = 103\n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state = RANDOM_SEED)\n",
    "    X_train = X_train.values\n",
    "    X_test  = X_test.values\n",
    "    X_original = df\n",
    "    X_original = X_original.values\n",
    "#======================\n",
    "#Data normalization\n",
    "#======================\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "    X_original_scaled = scaler.transform(X_original)\n",
    "    print(\"X train scaled shape\", X_train_scaled.shape, X_original_scaled.shape)\n",
    "#===========================\n",
    "#Autoencoder Model Building\n",
    "#===========================\n",
    "# No of Neurons in each Layer [9,6,3,2,3,6,9]\n",
    "# No of Neurons in each Layer [98,49,30,15,7,3,2,3,7,15,30,49,98]\n",
    "# No of Neurons in each Layer [154,70,35,17,8,4,2,4,8,17,35,70,98]\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 70\n",
    "\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "    encoder = Dense(encoding_dim, activation=\"tanh\",activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "    encoder = Dense(int(encoding_dim / 2), activation=\"tanh\")(encoder)\n",
    "    encoder = Dense(int(int(encoding_dim / 2) / 2), activation=\"tanh\")(encoder)\n",
    "    #encoder = Dense(int(int(int(encoding_dim / 2) / 2) / 2), activation=\"tanh\")(encoder)\n",
    "    #encoder = Dense(int(int(int(int(encoding_dim / 2) / 2) / 2) /2), activation=\"tanh\")(encoder)\n",
    "    #encoder = Dense(int(2), activation=\"tanh\")(encoder)\n",
    "    #decoder = Dense(int(int(int(int(encoding_dim / 2) / 2) / 2) /2), activation='tanh')(encoder)\n",
    "    #decoder = Dense(int(int(int(encoding_dim / 2) / 2) / 2), activation='tanh')(decoder)\n",
    "    #decoder = Dense(int(int(encoding_dim / 2) / 2), activation='tanh')(decoder)\n",
    "    #decoder = Dense(int(encoding_dim/ 2), activation='tanh')(decoder)\n",
    "    decoder = Dense(int(encoding_dim), activation='tanh')(encoder)\n",
    "    decoder = Dense(input_dim, activation='tanh')(decoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.summary()\n",
    "\n",
    "#=======================\n",
    "#Training the model\n",
    "#=======================\n",
    "    nb_epoch = 100\n",
    "    batch_size = 50\n",
    "    autoencoder.compile(optimizer='adam', loss='mse' )\n",
    "    checkpointer = ModelCheckpoint(filepath=\"model.h5\", verbose=0, save_best_only=True)\n",
    "    \n",
    "    t_ini = datetime.datetime.now()\n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    history = autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                        epochs=nb_epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=1,\n",
    "                        callbacks=[checkpointer, tensorboard]\n",
    "                        )\n",
    "\n",
    "    t_fin = datetime.datetime.now()\n",
    "    print('Time to run the model: {} Sec.'.format((t_fin - t_ini).total_seconds()))\n",
    "    df_history = pd.DataFrame(history.history)\n",
    "\n",
    "    #=======================\n",
    "    #Prediction\n",
    "    #=======================\n",
    "\n",
    "    predictions = autoencoder.predict(X_original_scaled)\n",
    "    print(\"predictions shape:\", predictions.shape)\n",
    "    \n",
    "    diff = X_original_scaled - predictions\n",
    "\n",
    "    mse = np.mean(np.power(X_original_scaled - predictions, 2), axis=1)\n",
    "    #print(\"mse:\", mse)\n",
    "    \n",
    "    df_error = pd.DataFrame({'reconstruction_error': mse}, index=df.index)\n",
    "    print(\"df error:\", df_error.head())\n",
    "    print(\"Prediction finished.\")\n",
    "\n",
    "\n",
    "    #=======================\n",
    "    #select outliers\n",
    "    #=======================\n",
    "    outliers = df_error.index[df_error.reconstruction_error > 0.05].tolist()\n",
    "    #outliers\n",
    "    \n",
    "    df_error.to_csv(\"df_error_sigmoid_all_layers_without_outlier.csv\", index=True)\n",
    "    \n",
    "    \n",
    "    return outliers, df_error.reconstruction_error\n",
    "\n",
    "\n",
    "start_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"YOUR_PATH_TO_OUTLIERS_FILE.csv\", header=0, index_col='author_idauthor',\n",
    "                     usecols = ['author_idauthor', 'tReg','tReg1','tReg2','tReg3','tReg4',\n",
    "                               'tReg5', 'tReg6', 'tReg7', 'tReg8', 'tReg9', 'tReg10', 'tReg11', \n",
    "                                'tReg12','tReg13','hasListing', 'hasListing1', 'hasListing2', \n",
    "                                'hasListing3', 'hasListing4', 'hasListing5', 'hasListing6', 'hasListing7', \n",
    "                                'hasListing8', 'hasListing9', 'hasListing10', 'hasListing11','hasListing12', \n",
    "                                'hasListing13','hasReview', 'hasReview1', 'hasReview2', 'hasReview3', 'hasReview4', \n",
    "                                'hasReview5', 'hasReview6', 'hasReview7', 'hasReview8', 'hasReview9', 'hasReview10', \n",
    "                                'hasReview11', 'hasReview12','hasReview13',\n",
    "                                'isMask', 'isMask1', 'isMask2', 'isMask3', 'isMask4', 'isMask5', 'isMask6', 'isMask7', \n",
    "                                'isMask8', 'isMask9', 'isMask10', 'isMask11', 'isMask12', 'isMask13','followingDelta', 'followingDelta1', 'followingDelta2', \n",
    "                                'followingDelta3', 'followingDelta4', 'followingDelta5', 'followingDelta6', \n",
    "                                'followingDelta7', 'followingDelta8', 'followingDelta9', 'followingDelta10', \n",
    "                                'followingDelta11', 'followingDelta12', 'followingDelta13', 'status', 'status1', \n",
    "                                'status2', 'status3', 'status4', 'status5', 'status6', 'status7', 'status8', \n",
    "                                'status9', 'status10', 'status11', 'status12', 'status13', 'successTrade', \n",
    "                                'successTrade1', 'successTrade2', 'successTrade3', 'successTrade4', \n",
    "                                'successTrade5', 'successTrade6', 'successTrade7', 'successTrade8', \n",
    "                                'successTrade9', 'successTrade10', 'successTrade11', 'successTrade12', 'successTrade13', \n",
    "                                'rating', 'rating1', 'rating2', 'rating3', 'rating4',\n",
    "                                'rating5', 'rating6', 'rating7', 'rating8', 'rating9', 'rating10', 'rating11', \n",
    "                                'rating12', 'rating13', 'unresolvedTxn', 'unresolvedTxn1', 'unresolvedTxn2', \n",
    "                                'unresolvedTxn3', 'unresolvedTxn4', 'unresolvedTxn5', 'unresolvedTxn6', 'unresolvedTxn7', \n",
    "                                'unresolvedTxn8', 'unresolvedTxn9', 'unresolvedTxn10', 'unresolvedTxn11', 'unresolvedTxn12',\n",
    "                                'unresolvedTxn13', 'label'])\n",
    "print(\"df\", df.shape, df.size, df.ndim, df.columns ,df.isna().sum() )\n",
    "#print(\"\\n\")\n",
    "#print(df.head())\n",
    "df[['followersDelta1*100']] = df[['followingDelta1']].apply(multiply_by_100)\n",
    "y_true = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compared = df\n",
    "df_compared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create index for row\n",
    "df2check = df_compared.reset_index(drop=True)\n",
    "print(df2check.head())\n",
    "print(df2check.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude label \n",
    "del df2check['label']\n",
    "#print(df2check.head())\n",
    "#df2check.set_index(\"author_idauthor\", inplace=True)\n",
    "df2check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply drop duplicates\n",
    "df2check.drop_duplicates()\n",
    "df2check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df w/o outlier reconstruction error\n",
    "df_error_file = pd.read_csv(\"df_error_sigmoid_all_layers_without_outlier.csv\")\n",
    "print(df_error_file.head())\n",
    "print(\"mean is:\", df_error_file['reconstruction_error'].mean())\n",
    "print(\"std is:\", df_error_file['reconstruction_error'].std())\n",
    "sns.distplot(df_error_file.reconstruction_error, kde=True, hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find outliers >= x std()\n",
    "\n",
    "tprl = []\n",
    "fprl = []\n",
    "\n",
    "for i in range(0, 10, 1):\n",
    "    #    x = 0.2\n",
    "    #len(df_error_file[np.abs(df_error_file.reconstruction_error - df_error_file.reconstruction_error.mean()) \n",
    "    #                  >= (x*df_error_file.reconstruction_error.std())])\n",
    "\n",
    "    full_df = pd.read_csv(\"df_error_sigmoid_all_layers_without_outlier.csv\")\n",
    "    print(len(full_df))\n",
    "\n",
    "    print(len(df_error_file))\n",
    "    \n",
    "    x = i ;\n",
    "\n",
    "    # define reconstruction error mean, std, threshold\n",
    "    reconstruction_error_mean = df_error_file.reconstruction_error.mean()\n",
    "    reconstruction_error_std = df_error_file.reconstruction_error.std()\n",
    "    threshold = reconstruction_error_mean + x * reconstruction_error_std\n",
    "    threshold1 = reconstruction_error_mean - x * reconstruction_error_std\n",
    "    print(reconstruction_error_mean, reconstruction_error_std, threshold, threshold1)\n",
    "\n",
    "    # predict all labels based on threshold\n",
    "    #full_df['predicted'] = full_df['reconstruction_error'].apply(lambda x: 1 if x >= threshold else 0 )\n",
    "    full_df['predicted'] = full_df['reconstruction_error'].apply(lambda x: 1 if (x >= threshold or x <= threshold1) else 0 )\n",
    "    print(len(full_df))\n",
    "\n",
    "    # construct df for evaluation\n",
    "    matrix_df = full_df[['predicted']]\n",
    "    test = pd.read_csv(\"YOUR_PATH_TO_OUTLIERS_FILE.csv\", header=0,usecols = ['label'])\n",
    "    matrix_df['label'] = test['label']\n",
    "    len(matrix_df)\n",
    "\n",
    "    cf = confusion_matrix(matrix_df['label'], matrix_df['predicted'])\n",
    "    print(cf)\n",
    "    tpr = cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "    fpr = cf[1,0]/(cf[1,0]+cf[1,1])\n",
    "\n",
    "    tprl.append(tpr)\n",
    "    fprl.append(fpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df = pd.DataFrame({\n",
    "    'tprl': tprl,\n",
    "    'fprl': fprl})\n",
    "\n",
    "sns.scatterplot(data=roc_df, x='tprl', y='fprl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tprl\", tprl)\n",
    "print(\"fprl\",fprl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(matrix_df['label'], matrix_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================\n",
    "#connection\n",
    "#=======================\n",
    "#from datetime import datetime\n",
    "import time\n",
    "while True:\n",
    "    try:\n",
    "        connection = mysql.connector.connect(host=DB_HOST,\n",
    "                                         port=DB_PORT,\n",
    "                                         database=DB_SCHEMA,\n",
    "                                         user=DB_USER,\n",
    "                                         password=DB_PWD)\n",
    "#===================================\n",
    "#check timeseries completion time and autoencoder cutoff date\n",
    "#===================================\n",
    "        sql_select_Query = '''SELECT * FROM site_dev.config where name=\"Autoencoder_processed_switch\" or name=\"Timeseries_max_processed_idauthor\" order by name asc'''\n",
    "    \n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(sql_select_Query)\n",
    "        results_config = cursor.fetchall()\n",
    "        #print(\"Total number of rows in site_dev.config is: \", cursor.rowcount)\n",
    "        #print(\"\\nPrinting each record\")\n",
    "        #print(results_config[0][0],results_config[1][0],results_config[1][1],results_config[1][2],results_config[0][2])\n",
    "        for column in results_config:\n",
    "            print(\"name = \", column[0])\n",
    "            print(\"value = \", column[1])\n",
    "            print(\"description  = \", column[2])\n",
    "#==============================\n",
    "#Check the autoencoder switch\n",
    "#==============================            \n",
    "        if results_config[0][2]!=results_config[1][2]:    \n",
    "             sql_update_Query = '''UPDATE site_dev.config SET value = 0, description = %s WHERE name='Autoencoder_processed_switch';'''    \n",
    "             #cursor.execute(sql_update_Query,(datetime.datetime.now().strftime(\"%Y%m%d\"),))\n",
    "             cursor.execute(sql_update_Query,(results_config[1][2],))\n",
    "             connection.commit()\n",
    "#===================================\n",
    "#if timeseries construction has done, autoencoder process starts\n",
    "#===================================\n",
    "        #elif results_config[1][1]=='0' and results_config[1][2]==datetime.datetime.now().strftime(\"%Y%m%d\") and results_config[0][1]=='0' and results_config[0][2]==datetime.datetime.now().strftime(\"%Y%m%d\"):\n",
    "        elif results_config[1][1]=='0' and results_config[1][2]==results_config[0][2] and results_config[0][1]=='0':\n",
    "            #print(results_site_idsite)\n",
    "            outliers,reconstruction_error=start_calculation()\n",
    "            for i in range(len(reconstruction_error)):\n",
    "                #print(reconstruction_error.index[i],reconstruction_error.iloc[i])\n",
    "                sql_select2_Query = 'SELECT a.idauthor, a.site_idsite FROM site_dev.author a join site_dev.author_profile_timeseries_test aptt on a.idauthor=aptt.author_idauthor where a.idauthor= ' + str(reconstruction_error.index[i])\n",
    "                cursor.execute(sql_select2_Query)\n",
    "                results_site_idsite = cursor.fetchall()\n",
    "                #print(\"idsite result: \", results_site_idsite[0][1])\n",
    "                author_idauthor=int(reconstruction_error.index[i])\n",
    "                calScore=float(reconstruction_error.iloc[i])\n",
    "                site_idsite=int(results_site_idsite[0][1])\n",
    "                tcreated=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                if reconstruction_error.iloc[i]>0.08:    \n",
    "                    #print(\"This is an outlier.\\n author_idauthor: \", author_idauthor,\"\\ncalScore: \", calScore)\n",
    "                    sql_insert_Query = '''INSERT INTO site_dev.author_outliers_test (author_idauthor, calScore, falseAlarm, site_idsite,tcreated,createdby) VALUES (%s, %s, 0, %s, %s,'system')'''\n",
    "                    cursor.execute(sql_insert_Query,(author_idauthor,calScore,site_idsite,tcreated))\n",
    "                    connection.commit()\n",
    "                    print(\"Inserted Succesfully. This is Outliers.\", \"\\nauthor_idauthor: \", author_idauthor, \"\\ncalScore: \", calScore, \"\\nsite_idsite: \", site_idsite, \"\\ntcreated: \", tcreated)\n",
    "                else:\n",
    "                    sql_insert_Query = '''INSERT INTO site_dev.author_outliers_test (author_idauthor, calScore, falseAlarm, site_idsite,tcreated,createdby) VALUES (%s, %s, 1, %s, %s,'system')'''\n",
    "                    cursor.execute(sql_insert_Query,(author_idauthor,calScore,site_idsite,tcreated))\n",
    "                    connection.commit()\n",
    "                    print(\"Inserted Succesfully. This is Normal.\", \"\\nauthor_idauthor: \", author_idauthor, \"\\ncalScore: \", calScore, \"\\nsite_idsite: \", site_idsite, \"\\ntcreated: \", tcreated)\n",
    "#=========================================================\n",
    "#After processing, change switch to 1 and Date=timeseries config's date\n",
    "#=========================================================       \n",
    "            #sql_update2_Query = '''UPDATE site_dev.config SET value = 1, description = %s WHERE name='Autoencoder_processed_switch';'''    \n",
    "            sql_update2_Query = '''UPDATE site_dev.config SET value = 1 WHERE name='Autoencoder_processed_switch';'''\n",
    "            #cursor.execute(sql_update2_Query,(results_config[1][2],))\n",
    "            cursor.execute(sql_update2_Query)\n",
    "            connection.commit()\n",
    "            #print(\"outliers: \", outliers, \"df_recon_error: \", reconstruction_error)\n",
    "        else:\n",
    "            print('Autoencoding has already done today.')\n",
    "    except Error as e:\n",
    "        print(\"parameterized query failed {}\".format(e),\"\\nTry again in 20seconds\")\n",
    "        time.sleep(20)\n",
    "    finally:\n",
    "        if (connection.is_connected()):\n",
    "            connection.close()\n",
    "            cursor.close()\n",
    "            print(\"MySQL connection is closed\",\"Sleeping for 60seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#k means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv(r\"YOUR_PATH_TO_OUTLIERS_FILE.csv\", header=0, index_col='author_idauthor',\n",
    "                    usecols = ['author_idauthor', 'tReg','tReg1','tReg2','tReg3','tReg4',\n",
    "                               'tReg5', 'tReg6', 'tReg7', 'tReg8', 'tReg9', 'tReg10', 'tReg11', \n",
    "                                'tReg12','tReg13','hasListing', 'hasListing1', 'hasListing2', \n",
    "                                'hasListing3', 'hasListing4', 'hasListing5', 'hasListing6', 'hasListing7', \n",
    "                                'hasListing8', 'hasListing9', 'hasListing10', 'hasListing11','hasListing12', \n",
    "                                'hasListing13','hasReview', 'hasReview1', 'hasReview2', 'hasReview3', 'hasReview4', \n",
    "                                'hasReview5', 'hasReview6', 'hasReview7', 'hasReview8', 'hasReview9', 'hasReview10', \n",
    "                                'hasReview11', 'hasReview12','hasReview13',\n",
    "                                'isMask', 'isMask1', 'isMask2', 'isMask3', 'isMask4', 'isMask5', 'isMask6', 'isMask7', \n",
    "                                'isMask8', 'isMask9', 'isMask10', 'isMask11', 'isMask12', 'isMask13','followingDelta', 'followingDelta1', 'followingDelta2', \n",
    "                                'followingDelta3', 'followingDelta4', 'followingDelta5', 'followingDelta6', \n",
    "                                'followingDelta7', 'followingDelta8', 'followingDelta9', 'followingDelta10', \n",
    "                                'followingDelta11', 'followingDelta12', 'followingDelta13', 'status', 'status1', \n",
    "                                'status2', 'status3', 'status4', 'status5', 'status6', 'status7', 'status8', \n",
    "                                'status9', 'status10', 'status11', 'status12', 'status13', 'successTrade', \n",
    "                                'successTrade1', 'successTrade2', 'successTrade3', 'successTrade4', \n",
    "                                'successTrade5', 'successTrade6', 'successTrade7', 'successTrade8', \n",
    "                                'successTrade9', 'successTrade10', 'successTrade11', 'successTrade12', 'successTrade13', \n",
    "                                'rating', 'rating1', 'rating2', 'rating3', 'rating4',\n",
    "                                'rating5', 'rating6', 'rating7', 'rating8', 'rating9', 'rating10', 'rating11', \n",
    "                                'rating12', 'rating13', 'unresolvedTxn', 'unresolvedTxn1', 'unresolvedTxn2', \n",
    "                                'unresolvedTxn3', 'unresolvedTxn4', 'unresolvedTxn5', 'unresolvedTxn6', 'unresolvedTxn7', \n",
    "                                'unresolvedTxn8', 'unresolvedTxn9', 'unresolvedTxn10', 'unresolvedTxn11', 'unresolvedTxn12',\n",
    "                                'unresolvedTxn13', 'label'])\n",
    "\n",
    "df[['followersDelta1*100']] = df[['followingDelta1']].apply(multiply_by_100)\n",
    "\n",
    "train_df = df.drop(['label'], axis=1)\n",
    "\n",
    "\n",
    "#======================\n",
    "#Data normalization\n",
    "#======================\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(train_df)\n",
    "#print(\"X scaled shape\", X_scaled.shape)\n",
    "\n",
    "#for i in range(10, 100, 10):\n",
    "#    k = i\n",
    "    \n",
    "#    print(\"k is\", k)\n",
    "#    kmeans = KMeans(n_clusters=k, random_state=0).fit(X_scaled)\n",
    "#    df['kmeans.labels_'] = kmeans.labels_\n",
    "#    print(\"value counts():\\n\", df['kmeans.labels_'].value_counts())\n",
    "#    output = df[df['label']==1]\n",
    "#    output.to_csv(str(k) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "tprl = []\n",
    "fprl = []\n",
    "\n",
    "\n",
    "X, y = train_df, df['label']\n",
    "clf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_params_)\n",
    "best_grid = grid_search.best_estimator_\n",
    "print(best_grid)\n",
    "\n",
    "df['predicted'] = grid_search.predict(train_df)\n",
    "cf = confusion_matrix(df['label'], df['predicted'])\n",
    "print(cf)\n",
    "tpr = cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "fpr = cf[1,0]/(cf[1,0]+cf[1,1])\n",
    "\n",
    "tprl.append(tpr)\n",
    "fprl.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tprl\", tprl)\n",
    "print(\"fprl\",fprl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tprl = []\n",
    "fprl = []\n",
    "\n",
    "X, y = X_scaled, df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "#print(y.head(),y_test.head())\n",
    "y_test_df = pd.DataFrame({'label': y_test}, index=y_test.index)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_df['predicted'] = clf.predict(X_test)\n",
    "cf = confusion_matrix(y_test_df['label'], y_test_df['predicted'])\n",
    "print(cf)\n",
    "tpr = cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "fpr = cf[1,0]/(cf[1,0]+cf[1,1])\n",
    "\n",
    "tprl.append(tpr)\n",
    "fprl.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tprl\", tprl)\n",
    "print(\"fprl\",fprl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tprl = []\n",
    "fprl = []\n",
    "\n",
    "X, y = X_scaled, df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#sm = SMOTE()\n",
    "#X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "#print(X_res, y_res)\n",
    "\n",
    "#print(y.head(),y_test.head())\n",
    "#y_test_df = pd.DataFrame({'label': y_test}, index=y_test.index)\n",
    "#clf = RandomForestClassifier()\n",
    "#clf.fit(X_res, y_res)\n",
    "#y_test_df['predicted'] = clf.predict(X_test)\n",
    "#cf = confusion_matrix(y_test_df['label'], y_test_df['predicted'])\n",
    "#print(cf)\n",
    "#tpr = cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "#fpr = cf[1,0]/(cf[1,0]+cf[1,1])\n",
    "\n",
    "#tprl.append(tpr)\n",
    "#fprl.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tprl\", tprl)\n",
    "print(\"fprl\",fprl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "print(Counter(y_over))\n",
    "\n",
    "...\n",
    "# define oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy=0.1)\n",
    "# fit and apply the transform\n",
    "X, y = over.fit_resample(X, y)\n",
    "# define undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "X, y = under.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_scaled, df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(XGBClassifier(), X_train, y_train)\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBClassifier results\n",
    "y_test_df = pd.DataFrame({'label': y_test}, index=y_test.index)\n",
    "y_test_df['predicted'] = model.predict(X_test)\n",
    "cf = confusion_matrix(y_test_df['label'], y_test_df['predicted'])\n",
    "print(cf)\n",
    "tpr = cf[0,0]/(cf[0,0]+cf[0,1])\n",
    "fpr = cf[1,0]/(cf[1,0]+cf[1,1])\n",
    "\n",
    "tprl.append(tpr)\n",
    "fprl.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tprl\", tprl)\n",
    "print(\"fprl\",fprl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
